{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# import os\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %pip install evaluate rouge_score faiss-cpu bert-score\n\n%pip install evaluate rouge_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:50:57.284598Z","iopub.execute_input":"2025-05-18T02:50:57.284957Z","iopub.status.idle":"2025-05-18T02:50:57.637081Z","shell.execute_reply.started":"2025-05-18T02:50:57.284931Z","shell.execute_reply":"2025-05-18T02:50:57.636312Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"694cd5d679c64b30accbe0703b1a6215"}},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"### Install Git-LFS to upload your model checkpoints","metadata":{}},{"cell_type":"code","source":"%%capture\n%apt install git-lfs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:51:24.030006Z","iopub.execute_input":"2025-05-18T02:51:24.030278Z","iopub.status.idle":"2025-05-18T02:51:24.034815Z","shell.execute_reply.started":"2025-05-18T02:51:24.030256Z","shell.execute_reply":"2025-05-18T02:51:24.034092Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# set device (GPU if available)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# prevent CUDA memory fragmentation issues\nos.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n\n# free up GPU memory before training\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:01:41.987833Z","iopub.execute_input":"2025-05-18T04:01:41.988533Z","iopub.status.idle":"2025-05-18T04:01:41.992685Z","shell.execute_reply.started":"2025-05-18T04:01:41.988508Z","shell.execute_reply":"2025-05-18T04:01:41.992097Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## **Import libraries**","metadata":{}},{"cell_type":"code","source":"import os\n\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nfrom datasets import load_dataset\nimport torch\nimport time\n\nimport evaluate\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# from rouge_score import rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:51:26.523271Z","iopub.execute_input":"2025-05-18T02:51:26.523531Z","iopub.status.idle":"2025-05-18T02:51:55.033493Z","shell.execute_reply.started":"2025-05-18T02:51:26.523509Z","shell.execute_reply":"2025-05-18T02:51:55.032923Z"}},"outputs":[{"name":"stderr","text":"2025-05-18 02:51:41.990808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747536702.224045      83 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747536702.288250      83 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Load dataset**","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"316usman/research_clinical_visit_note_summarization_corpus_mts\")\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:51:55.034771Z","iopub.execute_input":"2025-05-18T02:51:55.035273Z","iopub.status.idle":"2025-05-18T02:52:02.026597Z","shell.execute_reply.started":"2025-05-18T02:51:55.035253Z","shell.execute_reply":"2025-05-18T02:52:02.026034Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/525 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"911cd6a36c534cf2a888b8b2ca067ede"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/561k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f208c26fdee0498fb4bcb9fbf73beda2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/43.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"967d05ac7bf14b048305f2f7d107f86b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/182k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a20d93c4aa194799904c9cb6eb9d0442"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1201 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79729d55ea0b48d6b9c5f01bdeb045f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb9e7bffb0834c5c8f35d95554480585"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a91acfe97241ac8b6b3a9197212745"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'completion'],\n        num_rows: 1201\n    })\n    validation: Dataset({\n        features: ['prompt', 'completion'],\n        num_rows: 100\n    })\n    test: Dataset({\n        features: ['prompt', 'completion'],\n        num_rows: 400\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## **Load models and tokenizers**","metadata":{}},{"cell_type":"markdown","source":"### **T5-base**","metadata":{}},{"cell_type":"code","source":"# load T5 model\nt5_name ='google/flan-t5-base'\nt5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_name)\n\n# T5 tokenizer\n# parameter use_fast switches on fast tokenizer\nt5_tokenizer = AutoTokenizer.from_pretrained(t5_name, use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:42:48.439010Z","iopub.execute_input":"2025-05-18T03:42:48.439559Z","iopub.status.idle":"2025-05-18T03:43:19.577040Z","shell.execute_reply.started":"2025-05-18T03:42:48.439537Z","shell.execute_reply":"2025-05-18T03:43:19.576197Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9630d5f337a4a4da8228743d3f5657c"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8c23026a39944c2910db970a0e6b834"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3488e31ac7424a8983569db5219b00e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd510c5bb16445ba73c11d93ec551aa"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30732787e7454dce9c7cbd699b7fc1f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8740d751103045d5b9d16b234362f1bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"880f28b1184c479c8553cf0ef984ca93"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"MAX_LENGTH = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:44:41.502356Z","iopub.execute_input":"2025-05-18T03:44:41.502649Z","iopub.status.idle":"2025-05-18T03:44:41.506529Z","shell.execute_reply.started":"2025-05-18T03:44:41.502628Z","shell.execute_reply":"2025-05-18T03:44:41.505956Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"sample = ds['train']['prompt'][0]\nsample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:47:06.834208Z","iopub.execute_input":"2025-05-18T03:47:06.834471Z","iopub.status.idle":"2025-05-18T03:47:06.840759Z","shell.execute_reply.started":"2025-05-18T03:47:06.834451Z","shell.execute_reply":"2025-05-18T03:47:06.840174Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Doctor: What brings you back into the clinic today, miss? \\nPatient: I came in for a refill of my blood pressure medicine. \\nDoctor: It looks like Doctor Kumar followed up with you last time regarding your hypertension, osteoarthritis, osteoporosis, hypothyroidism, allergic rhinitis and kidney stones.  Have you noticed any changes or do you have any concerns regarding these issues?  \\nPatient: No. \\nDoctor: Have you had any fever or chills, cough, congestion, nausea, vomiting, chest pain, chest pressure?\\nPatient: No.  \\nDoctor: Great. Also, for our records, how old are you and what race do you identify yourself as?\\nPatient: I am seventy six years old and identify as a white female.'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"def preprocess_function(examples):\n    start_prompt = 'Summarize the following conversation doctor-patient conversation: \\n'\n\n    prompt = [start_prompt + dialogue for dialogue in examples[\"prompt\"]]\n    \n    examples['input_ids'] = t5_tokenizer(\n        prompt, \n        max_length=MAX_LENGTH,\n        padding=\"max_length\", \n        truncation=True, \n        return_tensors=\"pt\"\n    ).input_ids\n\n    # set up the tokenizer for targets\n    targets = [summary for summary in examples['completion']]\n    \n    examples['labels'] = t5_tokenizer(\n        targets, \n        max_length=MAX_LENGTH,\n        padding=\"max_length\", \n        truncation=True, \n        return_tensors=\"pt\"\n    ).input_ids\n\n    return examples\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:53:47.401963Z","iopub.execute_input":"2025-05-18T03:53:47.402615Z","iopub.status.idle":"2025-05-18T03:53:47.410547Z","shell.execute_reply.started":"2025-05-18T03:53:47.402583Z","shell.execute_reply":"2025-05-18T03:53:47.409644Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# preprocess dataset\nt5_tokenized_ds = ds.map(preprocess_function, batched=True)\n# t5_tokenized_ds = t5_tokenized_ds.remove_columns(['prompt', 'completion'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:54:57.487551Z","iopub.execute_input":"2025-05-18T03:54:57.488109Z","iopub.status.idle":"2025-05-18T03:54:58.678318Z","shell.execute_reply.started":"2025-05-18T03:54:57.488085Z","shell.execute_reply":"2025-05-18T03:54:58.677751Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1201 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5095d65eec414ef886c0351f16f0b82d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8211adb411504da6b9bd62a97ba34e86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b41465a3c3d499eae17a7200f376acb"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./flan_t5_summarization\",\n    per_device_train_batch_size=8,\n    auto_find_batch_size=True,\n    learning_rate=3e-5,\n    weight_decay=0.1,\n    num_train_epochs=4,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    # metric_for_best_model=\"rouge1\", \n    report_to=\"none\", # or \"wandb\" for experiment tracking\n    push_to_hub=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:03:23.813907Z","iopub.execute_input":"2025-05-18T04:03:23.814635Z","iopub.status.idle":"2025-05-18T04:03:23.847770Z","shell.execute_reply.started":"2025-05-18T04:03:23.814608Z","shell.execute_reply":"2025-05-18T04:03:23.847193Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# create a Trainer\ntrainer = Trainer(\n    model=t5_model,\n    tokenizer=t5_tokenizer,\n    args=training_args,\n    train_dataset=t5_tokenized_ds[\"train\"],\n    eval_dataset=t5_tokenized_ds[\"test\"], \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:27:21.755025Z","iopub.execute_input":"2025-05-18T04:27:21.755534Z","iopub.status.idle":"2025-05-18T04:27:21.972469Z","shell.execute_reply.started":"2025-05-18T04:27:21.755511Z","shell.execute_reply":"2025-05-18T04:27:21.971965Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_83/2363942018.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:27:25.778848Z","iopub.execute_input":"2025-05-18T04:27:25.779546Z","iopub.status.idle":"2025-05-18T04:48:24.084098Z","shell.execute_reply.started":"2025-05-18T04:27:25.779522Z","shell.execute_reply":"2025-05-18T04:48:24.083449Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='604' max='604' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [604/604 20:03, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.218000</td>\n      <td>0.237029</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.244100</td>\n      <td>0.233794</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.258100</td>\n      <td>0.232137</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.255500</td>\n      <td>0.231620</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=604, training_loss=0.2510682969101217, metrics={'train_runtime': 1205.0401, 'train_samples_per_second': 3.987, 'train_steps_per_second': 0.501, 'total_flos': 3289574321160192.0, 'train_loss': 0.2510682969101217, 'epoch': 4.0})"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# push best T5 model and tokenizer to HF\ntrainer.push_to_hub() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:09:40.243386Z","iopub.execute_input":"2025-05-18T05:09:40.243933Z","iopub.status.idle":"2025-05-18T05:09:49.240529Z","shell.execute_reply.started":"2025-05-18T05:09:40.243909Z","shell.execute_reply":"2025-05-18T05:09:49.239799Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/tn379/flan_t5_summarization/commit/ec3d85b7819a933120ce3a9042f580a934885539', commit_message='End of training', commit_description='', oid='ec3d85b7819a933120ce3a9042f580a934885539', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tn379/flan_t5_summarization', endpoint='https://huggingface.co', repo_type='model', repo_id='tn379/flan_t5_summarization'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":44},{"cell_type":"markdown","source":"### Tomorrow starts from here","metadata":{}},{"cell_type":"code","source":"# load fine-tuned model from HF\nlocal_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"tn379/flan_t5_summarization\")\nlocal_t5_tokenizer = AutoTokenizer.from_pretrained(\"tn379/flan_t5_summarization\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:09:54.587397Z","iopub.execute_input":"2025-05-18T05:09:54.587645Z","iopub.status.idle":"2025-05-18T05:10:24.671736Z","shell.execute_reply.started":"2025-05-18T05:09:54.587629Z","shell.execute_reply":"2025-05-18T05:10:24.671132Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a08eb528978f4c3b82fbf34fd2b7fef0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fb61184667c42199fa6d2902bad67b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a54918a601b4c3d98c3e1f562f3da14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f075a250e5804918a9ed9da16ee012a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e41e834865d45eb99da7093f2bc6dcb"}},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\nrouge = evaluate.load(\"rouge\")\n\ndef compute_rouge(predictions, references):\n    results = rouge.compute(\n        predictions=predictions,\n        references=references,\n        use_stemmer=True,\n        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"]\n    )\n    return {k: round(v, 4) for k, v in results.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:11:47.509671Z","iopub.execute_input":"2025-05-18T05:11:47.510214Z","iopub.status.idle":"2025-05-18T05:11:49.739898Z","shell.execute_reply.started":"2025-05-18T05:11:47.510189Z","shell.execute_reply":"2025-05-18T05:11:49.739311Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"601675db8d794de5bd6f30a209ca569a"}},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"from tqdm import tqdm\n\npredictions = []\nreferences = []\n\nfor example in tqdm(ds['validation']):\n    input_text = example[\"prompt\"]\n    reference = example[\"completion\"]\n\n    input_ids = local_t5_tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True).to(local_t5.device)\n    with torch.no_grad():\n        output_ids = local_t5.generate(input_ids, max_new_tokens=200)\n    pred = local_t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n    predictions.append(pred)\n    references.append(reference)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:12:14.017342Z","iopub.execute_input":"2025-05-18T05:12:14.017584Z","iopub.status.idle":"2025-05-18T05:17:01.631237Z","shell.execute_reply.started":"2025-05-18T05:12:14.017567Z","shell.execute_reply":"2025-05-18T05:17:01.630544Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:47<00:00,  2.88s/it]\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"results = compute_rouge(predictions, references)\nprint(\"ROUGE scores:\", results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:17:42.955731Z","iopub.execute_input":"2025-05-18T05:17:42.956002Z","iopub.status.idle":"2025-05-18T05:17:43.269490Z","shell.execute_reply.started":"2025-05-18T05:17:42.955984Z","shell.execute_reply":"2025-05-18T05:17:43.268624Z"}},"outputs":[{"name":"stdout","text":"ROUGE scores: {'rouge1': 0.3122, 'rouge2': 0.1302, 'rougeL': 0.2474}\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"for i in range(4):\n    print(f\"--- Example {i+1} ---\")\n    print(f\"Original Summary:\\n{references[i]}\\n\")\n    print(f\"Predicted Summary:\\n{predictions[i]}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:17:50.877417Z","iopub.execute_input":"2025-05-18T05:17:50.877702Z","iopub.status.idle":"2025-05-18T05:17:50.882233Z","shell.execute_reply.started":"2025-05-18T05:17:50.877680Z","shell.execute_reply":"2025-05-18T05:17:50.881390Z"}},"outputs":[{"name":"stdout","text":"--- Example 1 ---\nOriginal Summary:\nThe patient is a 26-year-old female, referred to Physical Therapy for low back pain.  The patient has a history of traumatic injury to low back.  The patient stated initial injury occurred eight years ago, when she fell at a ABC Store.  The patient stated she received physical therapy, one to two visits and received modality treatment only, specifically electrical stimulation and heat pack per patient recollection.  The patient stated that she has had continuous low-back pain at varying degrees for the past eight years since that fall.  The patient gave birth in August 2008 and since the childbirth, has experienced low back pain.  The patient also states that she fell four to five days ago, while mopping her floor.  The patient stated that she landed on her tailbone and symptoms have increased since that fall.  The patient stated that her initial physician examination with Dr. X was on 01/10/09, and has a followup appointment on 02/10/09.\n\nPredicted Summary:\nThe patient is a 26-year-old female who has had low back pain for about 8 years. She was referred to P T, and she went, but only once or twice, and they only did the electrical stimulation, and heat. She has had a history of back pain since giving birth. She had a son in August of 2008, and has had back pain since giving birth. She fell four or five days ago while mopping the floor. She landed on her lower back again, right onto her tailbone. She has had a follow up appointment scheduled for February 10th, 2009.\n\n--- Example 2 ---\nOriginal Summary:\nAs mentioned denies any oropharyngeal swelling.  No lip or tongue swelling.  No wheezing or shortness of breath.  No headache.  No nausea.  Notes itchy rash, especially on his torso and upper arms.\n\nPredicted Summary:\nThe patient is a 59-year-old male who presents today with a rash on his upper arms and torso. He has had a rash on his upper arms and torso for a day or two. He has had a lot of itchiness to the area. He has had episodes of shortness of breath or wheezing. He has had swelling to his lips, tongue, or throat. He has had no nausea or headaches. He has not applied any topical cream to the rash.\n\n--- Example 3 ---\nOriginal Summary:\nEssentially unchanged from her visit of 04/15/2005.\n\nPredicted Summary:\nThe patient has not been diagnosed with any new medical conditions or any new symptoms since April 15th, 2005.\n\n--- Example 4 ---\nOriginal Summary:\nAccutane.\n\nPredicted Summary:\nAccutane has been a good treatment for acne.\n\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"### **Use PEFT/LORA on T5**","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, TaskType\n\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM, \n    inference_mode=False, \n    r=8, \n    lora_alpha=32, \n    lora_dropout=0.1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# bert = evaluate.load(\"bertscore\")\n\n# bert_res = bert.compute(predictions=predictions, references=references, lang=\"en\")\n\n# avg_bertscore_f1 = sum(bert_res[\"f1\"]) / len(bert_res[\"f1\"])\n\n\n# print(f\"Avg BERTScore F1: {avg_bertscore_f1:.4f}\")\n\n\n# # TODO: avg for precision, recall too","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %pip install git+https://github.com/google-research/bleurt.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from bleurt import score\n\n# # bleurt = evaluate.load(\"bleurt\", module_type=\"metric\")\n\n\n# # predictions = [\"hello there\", \"general kenobi\"]\n# # references = [\"hello there\", \"general kenobi\"]\n# bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=\"BLEURT-20\")\n# results = bleurt.compute(predictions=predictions, references=references)\n\n\n\n# # checkpoint = \"bleurt/BLEURT-20\"\n\n# # scorer = score.BleurtScorer(checkpoint)\n# # scores = scorer.score(references=references, candidates=predictions)\n# # assert isinstance(scores, list) and len(scores) == 1\n# # print(scores)\n\n# # bleurt_res = evaluate.load(\"bleurt\", checkpoint=\"bleurt-20\")\n\n# # avg_bleurt = sum(bleurt_res[\"scores\"]) / len(bleurt_res[\"scores\"])\n# # print(f\"Avg BLEURT: {avg_bleurt:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(results, sum(results['scores'])/len(results['scores'])  )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **BART**","metadata":{}},{"cell_type":"code","source":"# load BART model\nbart_name = 'facebook/bart-large-cnn'\nbart_model = AutoModelForSeq2SeqLM.from_pretrained(bart_name)\n\n# BART tokenizer\nbart_tokenizer = AutoTokenizer.from_pretrained(bart_name, use_fast=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **EDA**","metadata":{}},{"cell_type":"code","source":"train_ds = ds['train']\nval_ds = ds['validation']\ntest_ds = ds['test']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# token length computation function\ndef compute_token_lengths(example):\n    dialogue = example['prompt']\n    summary = example['completion']\n    \n    # T5 tokenizer\n    dialogue_len_t5 = len(t5_tokenizer(dialogue, truncation=True, padding=False)['input_ids'])\n    summary_len_t5 = len(t5_tokenizer(summary, truncation=True, padding=False)['input_ids'])\n    \n    # BART tokenizer\n    dialogue_len_bart = len(bart_tokenizer(dialogue, truncation=True, padding=False)['input_ids'])\n    summary_len_bart = len(bart_tokenizer(summary, truncation=True, padding=False)['input_ids'])\n    \n    return {\n        'dialogue_len_t5': dialogue_len_t5,\n        'summary_len_t5': summary_len_t5,\n        'dialogue_len_bart': dialogue_len_bart,\n        'summary_len_bart': summary_len_bart\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_split(split_name):\n    split_dataset = ds[split_name]\n    lengths_dataset = split_dataset.map(compute_token_lengths)\n    \n    dialogue_lengths_t5 = lengths_dataset['dialogue_len_t5']\n    summary_lengths_t5 = lengths_dataset['summary_len_t5']\n    dialogue_lengths_bart = lengths_dataset['dialogue_len_bart']\n    summary_lengths_bart = lengths_dataset['summary_len_bart']\n    \n    print(f\"\\n--- {split_name.upper()} ---\")\n    print(f\"T5 - Avg dialogue length: {np.mean(dialogue_lengths_t5):.2f}, Max: {np.max(dialogue_lengths_t5)}\")\n    print(f\"T5 - Avg summary length: {np.mean(summary_lengths_t5):.2f}, Max: {np.max(summary_lengths_t5)}\")\n    print(f\"BART - Avg dialogue length: {np.mean(dialogue_lengths_bart):.2f}, Max: {np.max(dialogue_lengths_bart)}\")\n    print(f\"BART - Avg summary length: {np.mean(summary_lengths_bart):.2f}, Max: {np.max(summary_lengths_bart)}\")\n    \n    # return dialogue_lengths_t5, summary_lengths_t5, dialogue_lengths_bart, summary_lengths_bart\n    return dialogue_lengths_t5, summary_lengths_t5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# process each split\nsplits = ['train', 'validation', 'test']\nall_lengths = {}\n\nfor split in splits:\n    all_lengths[split] = process_split(split)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(18, 12))\n\nfor idx, split in enumerate(splits):\n    dialogue_lengths_t5, summary_lengths_t5 = all_lengths[split]\n    \n    plt.subplot(3, 2, idx + 1)\n    plt.hist(dialogue_lengths_t5, bins=10, alpha=0.6, label='Dialogue Length')\n    plt.hist(summary_lengths_t5, bins=10, alpha=0.6, label='Summary Length')\n    plt.title(f'Token Lengths - {split.capitalize()}', fontsize=15)\n    plt.xlabel('Number of Tokens', fontsize=12)\n    plt.ylabel('Frequency', fontsize=12)\n    plt.legend()\n\n    # plt.subplot(3, 2, idx*2+2)\n    # plt.hist(dialogue_lengths_bart, bins=50, alpha=0.6, label='Dialogue Length (BART)')\n    # plt.hist(summary_lengths_bart, bins=50, alpha=0.6, label='Summary Length (BART)')\n    # plt.title(f'BART Token Lengths - {split.capitalize()}', fontsize=15)\n    # plt.xlabel('Number of Tokens', fontsize=12)\n    # plt.ylabel('Frequency', fontsize=12)\n    # plt.legend()\n\nplt.tight_layout()\nplt.savefig('token_plot_1.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plt.figure(figsize=(18, 12))\n\n# for idx, split in enumerate(splits):\n#     dialogue_lengths_t5, summary_lengths_t5, dialogue_lengths_bart, summary_lengths_bart = all_lengths[split]\n    \n#     plt.subplot(3, 2, idx*2+1)\n#     plt.hist(dialogue_lengths_t5, bins=50, alpha=0.6, label='Dialogue Length (T5)')\n#     plt.hist(summary_lengths_t5, bins=50, alpha=0.6, label='Summary Length (T5)')\n#     plt.title(f'T5 Token Lengths - {split.capitalize()}', fontsize=15)\n#     plt.xlabel('Number of Tokens', fontsize=12)\n#     plt.ylabel('Frequency', fontsize=12)\n#     plt.legend()\n\n#     plt.subplot(3, 2, idx*2+2)\n#     plt.hist(dialogue_lengths_bart, bins=50, alpha=0.6, label='Dialogue Length (BART)')\n#     plt.hist(summary_lengths_bart, bins=50, alpha=0.6, label='Summary Length (BART)')\n#     plt.title(f'BART Token Lengths - {split.capitalize()}', fontsize=15)\n#     plt.xlabel('Number of Tokens', fontsize=12)\n#     plt.ylabel('Frequency', fontsize=12)\n#     plt.legend()\n\n# plt.tight_layout()\n# plt.savefig('token_plot.png')\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert HF dataset to df\ntrain_df = pd.DataFrame(ds['train'])\nval_df = pd.DataFrame(ds['validation'])\ntest_df = pd.DataFrame(ds['test'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.concat([train_df, val_df, test_df])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Dataset shape: {df.shape}, \\nNull existing: {df.isnull().sum()}\")\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**=> No missing dialouges or summaries**","metadata":{}},{"cell_type":"markdown","source":"## **Train models on training set**","metadata":{}},{"cell_type":"markdown","source":"### **T5**","metadata":{}},{"cell_type":"code","source":"sample = train_ds['prompt'][0]\nsample","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def summarize(text, max_length=150, min_length=30, num_beams=4, early_stopping=True):\n#     \"\"\"\n#     Generates a summary for the given text using the T5 model.\n\n#     Args:\n#         text (str): The input text to summarize.\n#         max_length (int): Maximum length of the generated summary.\n#         min_length (int): Minimum length of the generated summary.\n#         num_beams (int): Number of beams for beam search decoding.\n#         early_stopping (bool): Whether to stop beam search early.\n\n#     Returns:\n#         str: The generated summary.\n#     \"\"\"\n#     input_text = \"summarize: \" + text\n#     input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n\n#     summary_ids = t5_model.generate(\n#         input_ids,\n#         max_length=max_length,\n#         min_length=min_length,\n#         num_beams=num_beams,\n#         early_stopping=early_stopping\n#     )\n\n#     summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n#     return summary\n\n# # Example usage:\n# article = \"\"\"\n# The Daman and Diu administration on Wednesday announced that all offices under the union territory will remain closed on January 26 on the occasion of Republic Day.\n# In a circular issued by the administration, all heads of offices/departments have been asked to ensure that the circular is brought to the notice of all concerned.\n# Republic Day is celebrated every year on January 26 to commemorate the day when the Constitution of India came into effect in 1950.\n# \"\"\"\n# summary = summarize(sample)\n# print(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tokenize function\ndef t5_tokenize_function(example):\n    start_prompt = 'Summarize the following conversation.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"prompt\"]]\n\n    example['input_ids'] = t5_tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = t5_tokenizer(example[\"completion\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n\n    return example","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# apply tokenization\nt5_tokenized_ds = ds.map(t5_tokenize_function, batched=True)\nt5_tokenized_ds = t5_tokenized_ds.remove_columns(['prompt', 'completion'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_dir=\"./results\"\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-5,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    per_device_train_batch_size=8,\n    auto_find_batch_size=True,\n    logging_steps=10,\n    # max_steps=1,\n    eval_strategy='epoch',\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=t5_model,\n    tokenizer=t5_tokenizer,\n    args=training_args,\n    train_dataset=t5_tokenized_ds['train'],\n    eval_dataset=t5_tokenized_ds['test']\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t5_checkpoint = \"/kaggle/working/results/checkpoint-302\"\nt5_instruct_model = AutoModelForSeq2SeqLM.from_pretrained(t5_checkpoint)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluate T5 Qualitatively\nindex = 100\ndialogue = test_ds[index]['prompt']\nhuman_baseline_summary = test_ds[index]['completion']\n\nprompt = f\"\"\"\nWrite a clinical note for this doctor-patient dialogue.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move model to device\nt5_model = t5_model.to(device)\n\n# Tokenize input and move to device\ninput_ids = t5_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n# Generate output\nt5_res = t5_model.generate(\n    input_ids=input_ids,\n    generation_config=GenerationConfig(\n        max_new_tokens=200, \n        num_beams=1, \n        pad_token_id=0,\n        eos_token_id=1,\n        decoder_start_token_id=0\n    )\n)\n\n# Decode result\nt5_text_res = t5_tokenizer.decode(t5_res[0], skip_special_tokens=True)\n\n# ==================== END OF T5 RESULT =================== #\n\n\n# input_ids = t5_tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# t5_res = t5_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n# t5_text_res = t5_tokenizer.decode(t5_res[0], skip_special_tokens=True)\n\n# Move model to device\nt5_instruct_model = t5_instruct_model.to(device)\n\nt5_instruct_res = t5_instruct_model.generate(\n    input_ids=input_ids, \n    generation_config=GenerationConfig(\n        max_new_tokens=200, \n        num_beams=1,\n        pad_token_id=0,\n        eos_token_id=1,\n        decoder_start_token_id=0\n    ))\n\nt5_instruct_text_res = t5_tokenizer.decode(t5_instruct_res[0], skip_special_tokens=True)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{t5_text_res}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{t5_instruct_text_res}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Tune T5 with PEFT**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **BART**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **In-context learning**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **PREVIOUS VERSION**","metadata":{}},{"cell_type":"markdown","source":"## **Extract embeddings from train set**","metadata":{}},{"cell_type":"markdown","source":"### Embed and Index train set","metadata":{}},{"cell_type":"code","source":"sample = train_ds['prompt'][0]\nsample","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport faiss\n\n# load a pretrained Sentence Transformer model \nembedder = SentenceTransformer('all-MiniLM-L6-v2')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# calculate embeddings by calling model.encode()\nembeddings = embedder.encode(sample)\nprint(embeddings.shape)\n\n# calculate the embedding similarities\nsimilarities = embedder.similarity(embeddings, embeddings)\nprint(similarities)\n\n\n# train_cnv = [example['prompt'] for example in train_ds]\n\n# # compute embeddings\n# train_embd = embedder.encode(train_cnv, convert_to_numpy=True)\n\n# # FAISS index\n# embd_dim = train_embd.shape[1]\n# index = faiss.IndexFlatL2(embd_dim)\n# index.add(train_embd)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# retrieval function\ndef retrieve_similar_examples(test_conversation, k=1):\n    test_embedding = embedder.encode([test_conversation], convert_to_numpy=True)\n    distances, indices = index.search(test_embedding, k)\n    \n    examples = [train_ds[int(idx)] for idx in indices[0]]\n    return examples","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Build RAP","metadata":{}},{"cell_type":"code","source":"def build_rap_prompt(test_conversation, k=1):\n    examples = retrieve_similar_examples(test_conversation, k=k)\n    \n    prompt = \"\"\n    for ex in examples:\n        prompt += f\"Conversation:\\n{ex['prompt']}\\nSummary:\\n{ex['completion']}\\n\\n\"\n\n    prompt += f\"Now summarize:\\nConversation:\\n{test_conversation}\\nSummary:\"\n    return prompt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run inference","metadata":{}},{"cell_type":"code","source":"def summarize_with_rap(test_conversation, tokenizer, model, k=1, max_length=256):\n    prompt = build_rap_prompt(test_conversation, k=k)\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n    outputs = model.generate(**inputs, max_length=max_length)\n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_example = test_ds[0]['prompt']\n\n# one-shot\nsummary = summarize_with_rap(test_example, tokenizer=t5_tokenizer, model=t5_model, k=1)\nprint(summary)\n\n# few-shot\nsummary_fewshot = summarize_with_rap(test_example, tokenizer=t5_tokenizer, model=t5_model, k=3)\nprint(summary_fewshot)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nFor future use\n# define a preprocessing function\n# def preprocess_function(examples):\n#     inputs = [\"summarize: \" + doc for doc in examples[\"prompt\"]] \n#     \"\"\"\n#     model_inputs = t5_tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n\n#     # tokenize labels/desired summary with text_target\n#     # with t5_tokenizer.as_target_tokenizer():\n#     labels = t5_tokenizer(examples[\"completion\"], max_length=150, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n\n#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n#     return model_inputs\n#     \"\"\"\n#     examples['input_ids'] = t5_tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n#     examples['labels'] = t5_tokenizer(examples[\"completion\"], max_length=150, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n\n#     return examples\n\n# inputs = [f\"Summarize the following conversation: {conversation}\" for conversation in examples['prompt']]\n    \n    # model_inputs = t5_tokenizer(\n    #     inputs,\n    #     max_length=MAX_LENGTH,\n    #     truncation=True,\n    #     padding='max_length',\n    #     return_tensors=\"pt\"\n    # ).input_ids\n \n    # # Set up the tokenizer for targets\n    # targets = [summary for summary in examples['completion']]\n    # labels = t5_tokenizer(\n    #     targets,\n    #     max_length=MAX_LENGTH,\n    #     truncation=True,\n    #     padding='max_length',\n    #     return_tensors=\"pt\"\n    # ).input_ids\n \n    # model_inputs[\"labels\"] = labels[\"input_ids\"]\n    # return model_inputs\n\n\n# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# t5_model.to(device)\n\n# # define data collator\n# # data_collator = DataCollatorForSeq2Seq(t5_tokenizer, model=t5_model)\n\n# # define evaluation metric\n# rouge = evaluate.load(\"rouge\")\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}